= Integration with OpenStack Nova
Jianwei Zuo <740684863@qq.com>
v0.1, 2020-05-17
:toc: right
:imagesdir: ../../images

NOTE: This document is under development

== Overview

OpenStack allows users to launch VM instance via Horizon Dashboard or run nova boot command via OpenStack CLI.
When it occurs, Nova compute would trigger a few REST calls to Neutron to allocate necessary network resources.
We would want to integrate with Nova to support the same set of user operations, especially the vm creation scenario.

[#system-requirements]
== Integration Requirements

. Identify all necessary changes (configuration and/or codes) in Nova to call Alcor REST APIs, instead of Neutron APIs
. Make a proposal that requires minimal Nova changes
. Implement the code/configuration change
. Deploy and test the change in a multi-node OpenStack cluster

== Review of OpenStack Workflow

Nova create a vm will through four stage.
1. check network options is ok
2. allocate network resource
3. rollback if fail

In scenario column on below table, tag by above scenario and features.

Nova call Neutron apis
[width="100%",cols="1,1,1,1,1,1,1", options="header"]
|====================
|Name |Method |Params |Fields |Neutron API |Alcor status |Scenario

|show_network
|get
|net_id
|provider: physical_network, provider: network_type
|/v2.0/networks/ {net_id}
|/project/{projectId}/ vpcs/{vpcId}
|stage 1: find a physical network not in multi-segments network

|show_network
|get
|net_id
|dns_domain
|/v2.0/networks/ {net_id}
|/project/{projectId}/ vpcs/{vpcId}
|stage 3: get network dns_domain for deallocate network

|show_network
|get
|net_id
|segments
|/v2.0/networks/ {net_id}
|/project/{projectId}/ vpcs/{vpcId}
|stage 1: find a first segment that provides a physical network in multi-segments network

|list_network
|get
|net_ids
|
|/v2.0/networks
|/project/{project_id}/ vpcs
|stage 1,2: get all request networks

|list_network
|get
|tenant_id: project_id, shared: False
|
|/v2.0/networks
|/project/{project_id}/ vpcs(no support)
|stage 1,2: get all tenant user not shared networks if request no networks

|list_network
|get
|tenant_id: project_id, shared: False, admin_state_up: True
|
|/v2.0/networks
|/project/{project_id}/ vpcs(no support)
|stage 1,2: get all tenant user not shared networks if request no networks and vm network is auto_allocate

|list_network
|get
|shared: True
|
|/v2.0/networks
|/project/{project_id}/ vpcs(no support)
|stage 1,2: get all shared networks for vm create, finals all_networks = available + shared=True

|list_subnets
|get
|id: [subnet_id]
|
|/v2.0/networks
|/project/{project_id}/ subnets
|stage 2: get subnet info from port if have request port and port exist

|show_port
|get
|
|
|/v2.0/ports/ {port_id}
|/project/{project_id}/ ports/{port_id}
|stage 1,2: get request port info, confirm port info is ok for instance

|show_port
|get
|
|binding_profile, network_id
|/v2.0/ports/ {port_id}
|/project/{project_id}/ ports/{port_id}
|stage 3: get port info for unbind port

|show_port
|get
|
|binding: vnic_type, network_id, binding_profile, resource_request
|/v2.0/ports/ {port_id}
|/project/{project_id}/ ports/{port_id}
|stage 1: retrieve port vNIC info

|list_ports
|get
|device_id: instance.uuid
|
|/v2.0/ports
|/project/{project_id}/ ports(no support)
|stage 3: get instance ports for deallocate resources

|list_ports
|get
|device_id: instance.uuid, tenant_id: project_id
|
|/v2.0/ports
|/project/{project_id}/ ports (not support)
|stage 2: get ports info for build network resource

|list_ports
|get
|network_id: net_id, device_owner: network:dhcp
|
|/v2.0/ports
|/project/{project_id}/ ports(no support)
|stage 2: get dhcp ports info if have request port and port exist in network

|list_ports
|get
|network_id: net_id, fixed_ips: ip_addrs
|device_id
|/v2.0/ports
|/project/{project_id}/ ports(no support)
|stage 1: confirm request ip address not in use

|create_port
|post
|port: {device_id: instance.uuid,
fixed_ips: {ip_address: fixed_ip},
network_id: net_id,
admin_state_up: True,
tenant_id: project_id,
security_groups:{}}
|
|/v2.0/ports
|/project/{project_id}/ ports
|stage 2: create port for instance

|update_port
|put
|port: {device_id: '',
device_owner: '',
'binding:host_id': None,
'binding:profile': {},
dns_name:''}
|
|/v2.0/ports/ {port_id}
|/project/{project_id}/ ports/{port_id}
|stage 4: unbind instance port

|update_port
|put
|port: {dns_name: ''}
|
|/v2.0/ports/ {port_id}
|/project/{project_id}/ ports/{port_id}
|stage 4: reset port dns name

|update_port
|put
|port: {'binding:host_id': host, device_owner: 'compute:zone', 'binding:profile':{} }
|
|/v2.0/ports/ {port_id}
|/project/{project_id}/ ports/{port_id}
|stage 2: update port binding for instance in build network resource(have request port id and port exist in network)

|update_port
|put
|port: {'binding:host_id': host, device_owner: 'compute:zone', 'binding:profile':{}, 'dns_name': network.dns_domain or instance.hostname, mac_address: mac}
|
|/v2.0/ports/ {port_id}
|/project/{project_id}/ ports/{port_id}
|stage 2: update port for instance in build network resource

|delete_port
|delete
|
|
|/v2.0/ports/ {port_id}
|/project/{project_id}/ ports/{port_id}
|stage 3;delete port for instance

|list_floatingips
|get
|fixed_ip_address: fixed_ip, port_id: port_id
|
|/v2.0/floatingips
|
|stage 2: get port floatingip if have request port and request port is exist

|show_quota
|get
|
|
|/v2.0/quotas/ {project_id}
|
|stage 1: confirm tenant user have enough ports resources

|list_extensions
|get
|
|return example: {'extensions': {{'updated': "2017-07-17T10:00:00-00：00",
name: port_binding_ extended,
links: [],
alias: binding-extended,
description: "Expose port binding of a virtual port to external application"}}}
|/v2.0/extensions
|
|stage 1, 2: get all support extension options

|get_auto_allocated_ topology
|get
|
|
|/v2.0/auto-allocated-topology/ {project_id}
| optional api
| stage 2: auto allocate network if no request network and no available network

|list_security_ groups
|get
|tenant_id: project_id
|
|/v2.0/security-groups
|no support
| stage 2: process security groups for instance in build network resource
|====================

=== VM Creation Workflow
image::vm_create.png["VM creation workflow", width=1024, link="vm_create.png"]

== Required Changes
https://github.com/openstack/python-neutronclient[neutronclient project]

Nova use python-neutronclient to call Neutron apis.Only need to change neutronclient/v2.0/client:Client class.

== How Nova client identify Alcor server url
In OpenStack, there are a auth server Keystone, it can offer server url auth and endpoint catalog.  +
https://docs.openstack.org/mitaka/cli-reference/keystone.html[keystone online docs]

So Alcor need register endpoint in Keystone. +
Register:

```
$ OpenStack endpoint create --region RegionOne
network public http://<alcor_ip>:<port>

$ OpenStack endpoint create --region RegionOne
network internal http://<alcor_ip>:<port>

$ OpenStack endpoint create --region RegionOne
network admin http://<alcor_ip>:<port>
```

After register in Keystone, Nova can get Alcor endpoint from Keystone. No need to change Nova config file.


== Integration Proposal

. Microservice APIs should support multi params query, e.g., show/list actions
. Rename resource name in url path or add same resource name in url path
. Support "field" params in api and response body should have the "field" params content

There are two ways to integration with Nova:

1. Change python-neutronclient to call Alcor related api.

    advantages:
        1)easy to accomplish

    disadvantages:
        1）hard to maintain, Need change all neutornclient if a new change in Alcor api
        2）poor compatibility, Need replace all neutronclient when integration with new OpenStack environment

2. Make an adaption layer in Alcor to adapt Alcor related api to standard Neutron api.

    advantages:
        easy to maintain, all changes is in Alcor
        strong compatibility, easy to integration with other OpenStack components and environment

    disadvantages:
        need to add a new layer to adapt Neutron api

== Test Plan

TBD