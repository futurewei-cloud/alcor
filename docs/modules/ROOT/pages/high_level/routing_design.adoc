= Key System Flows
Eric Li <sze.li@futurewei.com>, Liguang Xie <lxie@futurewei.com>, Chun-Jen (James) Chung<cchung@futurewei.com>
v0.1, 2020-06-02
:toc: right
:imagesdir: ../../images

NOTE: This document is under development

== Architecture Overview

image::example-configuration.jpg[] 


== Requirements

We want to have a design that is:

. fast provision - port provision (enabling VMs <-> VMs within same or different subnet)
. scalable - needs to handle 65,000 ports in a VPC, and 1,00,000 machines in a region
. high performance - fast direct VM to VM communication (first packet may go through a gateway)
. lean - minimal resource overhead on host (e.g. namespace, bridges)
. fast migration - VMs migration should have network blackout time < a few seconds


== Neutron router issues before DVR

1. VM to VM with different subnets need to go through router in the network node.
2. Even VMs hosted on the same compute node needs to route through the virtual router if they belongs to different subnet.
3. All north-south traffic needs to route through the virtual router in the network node.
4. The virtual router in network node has become the bottleneck, and also single point of failure.

With DVR, an instance of virtual router is installed on the compute node as needed to service the VM, and addressed most of the issues mentioned above, although it comes at a cost of code and provision complexity.


== Design Options 1 (Current: Alcor DVR approach)

The prefered design would be a DVR solution which provides direct VM to VM cross subnet communication. It will support all neutron DVR features and have improvement on top of it. The advantages compare to Neutron DVR includes:

. Direct and not chatty Alcor controller to Alcor control agent communication
. Using a much leaner openflow rules for routing instead of using namespace and TCP/IP stack (more detail below)
. Openflow routing rules are installed on-demand and cleaned by after idle timer (e.g. 60s) which keeps the active openflow table entries small for fast look up
. The design enable scale and performance from ground up, addressing the biggest pain point on neutron


== Design Options 2 (Next: Gateway approach)

A simple gateway approach:

. only program the compute host for the new port and gateway
. install a default flow in the compute host to route all unknown traffic to a gateway
. when gateway saw the VM to VM communication (different network)
.. download and install the routing flow in the sender host
.. the flow would have a idle timeout like 60s
.. will need to download and install the corresponding NACL 
.. should this be trigger on the gateway or compute host? 
... compute host can do better bandwidth measurement locally and it is more distributed

Challenge - we need the compute host to be able to measure the bandwidth used one a VM to VM communication. Only when the bandwidth exceeds some threshold, then we will download the flow rules and NACL to support VM to VM direct communication. This way, we won't overwhelm Alcor server or Gateway to send down too many flows + NACL info.


== Route Rule implementation (iptables vs openflow)

We will move away from neutron's which requires resources for the namespace which loads a complete TCP/IP stack with ARP and routing table, which will remove the latency and host CPU consumption of using linux devices and TCP/IP stack, and overhead of using iptables. 

We will use the more modern openflow rules + openflow controller application to implement our virtual router, which eliminates the usage of TCP/IP stack. It consumes much less resource and provides high routing performance by providing routing support using Openflow rules only.

[TODO] Need numbers to justify the adventage once we have our POC implemented.


== Workflow on Router programming

[TODO] What are the steps to program our router/DVR, FIP manager, SNAT manager?


== Virtual Router Implementation

A virtual router will be implemented by ACA. When Alcor Controller sends down the goal state messages to compute hosts, ACA will create in memory virtual router object and also the (virtual) gateway ports connected to it. ACA will then configure routing rules in the virtual router object. 

To implementation in virtual router as DVR in compute node, it needs to have:

. virtual router object
. virtual gateway interface(s) connected
. host virtual router MAC that's unique in the region

To support L3 routing, ACA will program two sets of rules: the essential set, and the on-demand set.

=== Openflow Essential Rules

The Openflow essential rules are programmed as soon as virtual router information is pushed down to ACA regardless of traffic. We need them to support:

. Intra-subnet traffic (ports in the same subnet that doesn't need routing), send using NORMAL path
. Traffic destinated to one the virtual router port, first packet send to ACA to program the openflow rule
. ARP and ICMP responder so that controller doesn't need to handle it

=== Openflow On-Demand Rules

For inter-subnet L3 traffic between VMs, the first packet will be sent to controller since the on-demand openflow rules has not been programmed yet. This model is used based on the assumption that most VMs don't talk to each other in the cloud environment. Since we don't want to flood our openflow rule table with ton of entries with large scale setup. We have this on-demand model to program the needed rule when needed.

With the first packet sends to CONTROLLER, ACA is acting as the openflow controller and look up its router objects. ACA will find the matching router and then program the corresonding openflow rules on the local machine. Once the openflow rules have been programmed, ACA will simply send the first packet back to OVS to route using the on-demand openflow rule just programmed. 

In order to keep the set of openflow rules lean and small as we scale. The on-demand rule will have an idle timeout of 60s. That means all the ongoing traffic will keep the rule alive, but if there is no traffic hitting the on-demand rule for 60s. The particular on-demand will be removed and any new traffic will hit the essential first packet rule agent to perform the on-demand rule programming. The idle timeout of 60s is the default and can be configured in ACA.

=== ACA Virtual Router Packet Flow with openflow tables

==== Table Triage: (openflow table 0)

. if ARP, send to Table ARP Responder
. if ICMP, send to Table ICMP Responder
. We likely need to handle broadcast/multicast, by simply send to Normal path
. else send to Table Forwarding

[source,shell]
------------------------------------------------------------
table=50, priority=50,proto=‘arp’ actions=resubmit(,51) (to table ARP Responder)
table=50, priority=50,proto=‘icmp’ actions=resubmit(,52) (to table ICMP Responder)
table=50, priority=10,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=NORMAL (for multicast)
table=50, priority=10,dl_dst=ff:ff:ff:ff:ff:ff actions=NORMAL (for broadcast)
table=50, priority=0 actions=resubmit(,55) (to table Forwarding)
------------------------------------------------------------

==== Table ARP Responder: (openflow table 51)

. if local VLAN and ARP target IP matches an openflow rule, send ARP response
. else send to Normal path

[source,shell]
------------------------------------------------------------
table=51, priority=50,proto=‘arp’,dl_vlan=[VLAN tag],nw_dst=[Target IP] actions=

    ‘move:NXM_OF_ETH_SRC[]->NXM_OF_ETH_DST[],’ – Put the source MAC address of the request (The requesting VM) as the new reply’s destination MAC address

    ‘mod_dl_src:%(mac)s,’ – Put the requested MAC address of the remote VM as this message’s source MAC address

    ‘load:0x2->NXM_OF_ARP_OP[],’ – Put an 0x2 code as the type of the ARP message. 0x2 is an ARP response.

    ‘move:NXM_NX_ARP_SHA[]->NXM_NX_ARP_THA[],’ – Place the ARP request’s source hardware address (MAC) as this new message’s ARP target / destination hardware address

    ‘move:NXM_OF_ARP_SPA[]->NXM_OF_ARP_TPA[],’ – Place the ARP request’s source protocol / IP address as the new message’s ARP destination IP address

    ‘load:%(mac)->NXM_NX_ARP_SHA[],’ – Place the requested VM’s MAC address as the source MAC address of the ARP reply

    ‘load:%(ip)->NXM_OF_ARP_SPA[],’ – Place the requested VM’s IP address as the source IP address of the ARP reply

    ‘load:0->NXM_OF_IN_PORT[]‘ – Send the message back to the port it came from

table=51, priority=0 actions=strip_vlan,actions=NORMAL
------------------------------------------------------------

==== Table ICMP Responder: (openflow table 52)

. if local VLAN and ICMP target matches an openflow rule, send ICMP response
. else send to Normal path?

[source,shell]
------------------------------------------------------------
table=52, priority=50,proto=icmp,dl_vlan=[VLAN tag],nw_dst=[Target IP] actions= 
    ‘move:NXM_OF_IP_SRC[]->NXM_OF_IP_DST[],mod_nw_src:[Target IP],
        load:0xff->NXM_NX_IP_TTL[],load:0->NXM_OF_ICMP_TYPE[],load:0->NXM_OF_IN_PORT[]‘

table=52, priority=0 actions=NORMAL
------------------------------------------------------------

==== Table Forwarding: (openflow table 55)

. (on demand rule) if inter-subnet communication matches an openflow rule, perform L3 forwarding, programmed in last 60s
. (L3 essential rule) if segment ID and destination L3 subnet matches an openflow rule, send to ACA
. (L2 essential rule) if local vlan and local subnet matches an openflow rule, send to Normal path
. else send to Table External, this is traffic to external

[source,shell]
------------------------------------------------------------
(on demand rule)table=55, priority=50,dl_vlan=[VLAN tag of network 1],dl_dst=[mac of GW for network 1] actions=

    ‘strip_vlan,load:[VLAN tag of network 2->NXM_NX_TUN_ID[],‘ - Replace to network 2 VLAN tag

    ‘mod_dl_dst=[destination VM MAC]‘ – replace the GW mac to destination VM’s MAC 

    ‘actions=NORMAL‘

(L3 essential rule)table=55, priority=10,dl_vlan=[VLAN tag of network 1],dl_dst=[mac of GW for network 1] actions=CONTROLLER

(L2 essential rule)table=55, priority=10,dl_vlan=[VLAN tag of network 1], [match local subnet] actions = NORMAL

table=55, priority=0 actions=resubmit(,60) (to table External)
------------------------------------------------------------

==== Table External: (openflow table 60)

. TBD

=== Gateway port

In order for two virtual subnets/networks to communicate with each other, both subnets needs to have a gateway port connects to a router instance, similar to how physical network works. 

For a regular port used by VM/Container, the linux network device and OVS port is created by Nova agent on the compute node. For gateway port, ACA will create a virtual gateway port inside its virtual router implementation.


== E2E Packet flow DVR (Alcor openflow way)

image::cross_subnet_with_DVR.png[] 

=== Case 1: East-west for instances on different compute hosts on different networks

Project network 1

* Network: 192.168.1.0/24
* Gateway: 192.168.1.1 with MAC address MAC_G1

Project network 2

* Network: 192.168.2.0/24
* Gateway: 192.168.2.1 with MAC address MAC_G2
* VNI: VNI_NET_2

Compute node 1

* Instance 1: 192.168.1.11 with MAC address MAC_VM1 using project network 1
* Compute host 1 unique DVR mac is MAC_HOST1_DVR

Compute node 2

* Instance 2: 192.168.2.11 with MAC address MAC_VM2 using project network 2

prerequistite

. needed DVR instance(s) created in ACA
. DVR gw interface macs programmed as openflow rule to route traffic to ACA
. ACA has route programmed in all DVR instance(s)

In Compute Node 1

. Instance 1 sends a packet to instance 2
. Instance 1 tap interface forwards packet to br-int. The packet contains its gateway destination MAC_G1 because the destination resides on another network 
    .. (src mac = MAC_VM1, dest mac = MAC_G1)
. br-int adds VLAN tag for project network 1 because of OVS port setting
. br-int sends the packet to openflow Table 0 (Triage) to Table 50 (Packet Classifier)
. openflow Table 50 (Packet Classifier) sends the packet to Table 55 (Forwarding)
. openflow Table 55 (Forwarding) cannot find a matching on-demand rule, sends packet to ACA based on essential rule
. ACA matches the gateway port MAC_G1 and found the corresponding virtual router object, confirm it can route to project network 2
. ACA adds on-demand openflow rule to Table 55 (Forwarding) and sends the first packet back to br-int
. br-int routes to project network 2 based on the just added on-demand rule, replace the VLAN tag to project network 2, set src mac to MAC_G2 
    .. (src mac = MAC_HOST1_DVR, dest mac = MAC_VM2)
. For VxLAN/GRE project networks, br-int forwards the packet to br-tun
. br-tun matches vlan, wraps the packet in VxLAN or GRE tunnel and adds tag VNI_NET_2 to identify project network 2
. br-tun forwards the packet to compute node 2 via the tunnel interface

In Compute Node 2

. For VxLAN and GRE project networks, tunnel interface forwards the packet to br-tun
. br-tun matches VNI_NET_2, unwraps the packet and adds VLAN tag for project network 2
. br-tun matches source mac of MAC_HOST1_DVR, restore src mac to MAC_G2
    .. (src mac = MAC_G2, dest mac = MAC_VM2)
. br-int forwards the packet to tap interface on instance 2

Note: Return traffic follows similar steps in reverse except Compute Node 2 will be using its own local DVR to route from project network 2 to project network 1


=== Case 2: North-south for instances with a fixed IP address

External network

* Network: 10.213.0.0/24
* IP allocation 10.213.0.101 to 10.213.0.200

Project network 1

* Network: 192.168.1.0/24
* Gateway: 192.168.1.1 with MAC address MAC_G1
* SNAT interface: 192.168.1.3 SNAT interface on network node, with external IP 10.213.0.102

Compute node 1

* Instance 1: 192.168.1.11 using project network 1

In Compute Node 1

. Instance 1 sends a packet to an external host
. Instance 1 tap interface forwards packet to br-int. The packet contains destination mac MAC_G1 because the destination resides on another network
. br-int adds VLAN tag for project network 1
. br-int removes the VLAN tag and forwards the packet to its gateway mac MAC_G1 in DVR namespace
. DVR routes the packet to the ip of SNAT namespace in the network node
. For VxLAN/GRE project networks, br-int forwards the packet to br-tun
. br-tun wraps the packet in VxLAN or GRE tunnel and adds a tag (VNI) to identify project network 1
. br-tun forwards the packet to network node via the tunnel interface

In Network Node

. For VxLAN and GRE project networks, tunnel interface forward the packet to br-tun
. br-tun unwraps the packet and adds VLAN tag for project network 1
. br-tun forwards the packet to br-int
. br-int removes VLAN tag and forwards the packet to SNAT namespace
. iptable service perform SNAT on the packet using its interface as the source IP
. SNAT namespace routes the packets to provider networks default gw, and forwards the packet to br-int
. br-int adds VLAN tag and forwards the packet to br-ex
. br-ex swaps internal VLAN tag to actual VLAN tag, and forwards the packet to external network via the external interface

[TODO] discuss a new design without using network node, use a shared external IP, and change to openflow way.

Note: Return traffic follows similar steps in reverse

=== Case 3: North-south for instances with a floating IP address

External network

* Network: 10.213.0.0/24
* IP allocation 10.213.0.101 to 10.213.0.200
* Network router interface 10.213.0.101

[Question] how can external traffic can be routed to 10.213.0.101 even for floating IP 10.213.0.102?
[Answer] FIP namespace does proxy arp to response to any arp request for any floating IP addresses including 10.213.0.102.

Project network 1

* Network: 192.168.1.0/24
* Gateway: 192.168.1.1 with MAC address MAC_G1

Compute node 1

* Instance 1: 192.168.1.11 using project network 1, and floating IP 10.213.0.102

In Compute Node 1

. an external host sends a packet to instance 1 using its floating IP 10.213.0.102
. external interface forwards the packet to br-ex
. br-ex swaps actual VLAN tag with internal VLAN tag, and forwards the packet to br-int
. br-int removes VLAN tag and forwards the packet FIP namespace, this interface responds to any ARP requests for the instance floating IPv4 address
. FIP namespace routes the packet to DVR namespace to qg in qrouter namespace, since qg contains instance 1 floating IP 10.213.0.102
. iptable service perform DNAT on the packet from instance 1 floating IP 192.168.1.11 to fixed IP 192.168.1.11, using the destination mac of instance 1
. qrouter namespace forwards the packet to br-int via qr-1 since it contains the project network 1 gateway IP 192.168.1.1 with MAC_G1
. br-int adds the VLAN tag for project network 1, and forwards the packet to tap interface on instance 1

[TODO] close on this design, and change to openflow way.

Note: Return traffic follows similar steps in reverse, but the network node performs SNAT on traffic passing from instance to external network. Below is the complete flow:

In Compute Node 1

. Instance 1 sends a packet to an external host
. Instance 1 tap interface forwards packet to br-int. The packet contains destination mac MAC_G1 because the destination resides on another network
. br-int adds VLAN tag for project network 1, and sends to packet to DVR namespace which contains mac MAC_G1
. iptable service perform SNAT on the packet from instance 1 fixed IP 192.168.1.11 to 10.213.0.102
. DVR namespace routes the packet to FIP namespace via a direct veth pair
. FIP namespace routes the packet to 10.213.0.1, which is the default gw for provider network, sends it to br-int
. br-int switches the traffic to br-ex
. br-ex internal VLAN tag is stripped and replaced with actual VLAN tag used in provider network

== E2E Packet flows without DVR (neutron namespace/iptable way for reference)

=== Case 1: East-west for instances on different compute hosts on different networks

Project network 1

* Network: 192.168.1.0/24
* Gateway: 192.168.1.1 with MAC address MAC_G1

Project network 2

* Network: 192.168.2.0/24
* Gateway: 192.168.2.1 with MAC address MAC_G2

Compute node 1

* Instance 1: 192.168.1.11 using project network 1

Compute node 2

* Instance 2: 192.168.2.11 using project network 2

In Compute Node 1

. Instance 1 sends a packet to instance 2
. Instance 1 tap interface forwards packet to br-int. The packet contains destination mac MAC_G1 because the destination resides on another network
. br-int adds VLAN tag for project network 1
. For VxLAN/GRE project networks, br-int forwards the packet to br-tun
. br-tun wraps the packet in VxLAN or GRE tunnel and adds a tag (VNI) to identify project network 1
. br-tun forwards the packet to network node via the tunnel interface

In Network Node

. For VxLAN and GRE project networks, tunnel interface forward the packet to br-tun
. br-tun unwraps the packet and adds VLAN tag for project network 1
. br-tun forwards the packet to br-int
. br-int removes the VLAN tag and forwards the packet to qr-1 on qrouter namespace, since qr-1 contains the project network 1 gateway IP 192.168.1.1 with MAC_G1
. qrouter namespace routes packet to qr-2 which contains project network 2 gateway IP 192.168.2.1 with MAC_G2
. qrouter namespace forwards the packet to br-int
. br-int adds the VLAN tag for project network 2
. For VxLAN/GRE project networks, br-int forwards the packet to br-tun
. br-tun wraps the packet in VxLAN or GRE tunnel and adds a tag (VNI) to identify project network 1
. br-tun forwards the packet to compute node 2 via the tunnel interface

In Compute Node 2

. For VxLAN and GRE project networks, tunnel interface forward the packet to br-tun
. br-tun unwraps the packet and adds VLAN tag for project network 2
. br-tun forwards the packet to br-int
. br-int forwards the packet to tap inetrface on instance 2

=== Case 2: North-south for instances with a fixed IP address

External network

* Network: 10.213.0.0/24
* IP allocation 10.213.0.101 to 10.213.0.200
* Network router interface 10.213.0.101

Project network 1

* Network: 192.168.1.0/24
* Gateway: 192.168.1.1 with MAC address MAC_G1

Compute node 1

* Instance 1: 192.168.1.11 using project network 1

In Compute Node 1

. Instance 1 sends a packet to an external host
. Instance 1 tap interface forwards packet to br-int. The packet contains destination mac MAC_G1 because the destination resides on another network
. br-int adds VLAN tag for project network 1
. For VxLAN/GRE project networks, br-int forwards the packet to br-tun
. br-tun wraps the packet in VxLAN or GRE tunnel and adds a tag (VNI) to identify project network 1
. br-tun forwards the packet to network node via the tunnel interface

In Network Node

. For VxLAN and GRE project networks, tunnel interface forward the packet to br-tun
. br-tun unwraps the packet and adds VLAN tag for project network 1
. br-tun forwards the packet to br-int
. br-int removes VLAN tag and forwards the packet to qr-1 on qrouter namespace, since qr-1 contains the project network 1 gateway IP 192.168.1.1 with MAC_G1
. iptable service perform SNAT on the packet using qg interface as the source IP, qg contains external network router interface 10.213.0.101, and send it to the gateway IP on the provider network
. qrouter namespace forwards the packet to br-int via qg
. br-int adds VLAN tag and forwards the packet to br-ex
. br-ex swaps internal VLAN tag to actual VLAN tag, and forwards the packet to external network via the external interface

Note: Return traffic follows similar steps in reverse.

=== Case 3: North-south for instances with a floating IP address

External network

* Network: 10.213.0.0/24
* IP allocation 10.213.0.101 to 10.213.0.200
* Network router interface 10.213.0.101

Project network 1

* Network: 192.168.1.0/24
* Gateway: 192.168.1.1 with MAC address MAC_G1

Compute node 1

* Instance 1: 192.168.1.11 using project network 1, and floating IP 10.213.0.102

In Network Node

. an external host sends a packet to instance 1 using its floating IP 10.213.0.102
. external interface forwards the packet to br-ex
. br-ex swaps actual VLAN tag with internal VLAN tag, and forwards the packet to br-int
. br-int forwards the packet to qg in qrouter namespace, since qg contains instance 1 floating IP 10.213.0.102
. iptable service perform DNAT on the packet with instance 1 fixed IP 192.168.1.11  
. qrouter namespace forwards the packet to br-int via qr-1 since it contains the project network 1 gateway IP 192.168.1.1 with MAC_G1
. br-int adds the VLAN tag for project network 1
. For VxLAN/GRE project networks, br-int forwards the packet to br-tun
. br-tun wraps the packet in VxLAN or GRE tunnel and adds a tag (VNI) to identify project network 1
. br-tun forwards the packet to compute node 1 via the tunnel interface

In Compute Node 1

. For VxLAN and GRE project networks, tunnel interface forward the packet to br-tun
. br-tun unwraps the packet and adds VLAN tag for project network 1
. br-tun forwards the packet to br-int
. br-int forwards the packet to tap interface on instance 1

Note: Return traffic follows similar steps in reverse, but the network node performs SNAT on traffic passing from instance to external network.


== L2 Population Optimization

It is an optional feature to prevent the flooding of ARP packet in the datacenter. Since Neutron server is aware of all virtual mac and virtual IP mappings, we can use that information to prepopluation forwarding entries on all tunnel bridges.

For scenario with two local ports on the same compute host, connected to the same br-int. The current design is to let the arp packet flood to br-tun, using the prepopulated br-tun's arp responder openflow rule. 


== Pending Items

. What happen if host crashed, do we just leverage the ovsdb stored data? Or we ask the Alcor controller for the whole set of configuration upon restart?

. How does Neutron router manages connection flows with HA router?


[bibliography]
== References

- [[[neutron-flows,1]]] https://docs.openstack.org/neutron/train/admin/deploy-ovs-selfservice.html
- [[[neutron-flows-old,2]]] https://docs.openstack.org/liberty/networking-guide/scenario-classic-ovs.html
- [[[neutron-dvr,3]]] https://docs.openstack.org/neutron/train/admin/deploy-ovs-ha-dvr.html