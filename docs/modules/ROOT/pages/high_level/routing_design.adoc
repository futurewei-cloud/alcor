= Key System Flows
Eric Li <sze.li@futurewei.com>, Liguang Xie <lxie@futurewei.com>, Chun-Jen (James) Chung<cchung@futurewei.com>
v0.1, 2020-06-02
:toc: right
:imagesdir: ../../images

NOTE: This document is under development

== Architecture Overview

Picture of different routing components including router/DVR, FIP manager, SNAT manager. Can use the picture from James.

image::ovs-dvr-architecture.jpg[] 


== Requirements

We want to have a design that is:

. fast provision - port provision (enabling VMs <-> VMs within same or different subnet)
. scalable - needs to handle 65,000 ports in a VPC, and 1,00,000 machines in a region
. high performance - fast direct VM to VM communication (first packet may go through a gateway)
. lean - minimal resource overhead on host (e.g. namespace, bridges)
. fast migration - VMs migration should have network blackout time < a few seconds


== Neutron implementation before DVR

Neutron routers has the following major issues:

1. VM to VM with different subnets need to go through router in the network node.
2. Even VMs hosted on the same compute node needs to route through the virtual router if they belongs to different subnet.
3. All north-south traffic needs to route through the virtual router in the network node.
4. The virtual router in network node has become the bottleneck, and also single point of failure.

With DVR, an instance of virtual router is installed on the compute node as needed to service the VM, and addressed almost of the issues above, although it comes at a cost of code and provision complexity.


== Design Options 1 (DVR approach)

[Eric] to add


== Design Options 2 (Gateway approach)

Can we start with a simple gateway approach where:

. only program the compute host for the new port and gateway
. install a default flow in the compute host to route all unknown traffic to a gateway
. when gateway saw the VM to VM communication (different network)
.. somehow install the flow in the sender host (how)
.. should the flow has a timeout like 300 mins (maybe not for now)
.. how to install the network ACL during the flow install? Maybe same approach as DVR
.. should this be trigger on the gateway or compute host? 
... compute host can do better bandwidth measurement when needed and it is more distributed

challenge - we need the compute host to be able to measure the bandwidth used one a VM to VM communication. Only when the bandwidth exceeds some threshold, then we will download the flow rules and NACL to support VM to VM direct communication. This way, we won't overwhelm Alcor server or Gateway to send down too many flow + NACL info.


== Route Rule implementation (iptables vs openflow)

We will move away from neutron's which requires resources for the namespace which loads a complete TCP/IP stack with ARP and routing table, which will remove the latency and host CPU consumption of using linux devices and TCP/IP stack, and overhead of using iptables. 

We will use the more modern openflow rules + openflow controller application to implement our virtual router, which eliminates the usage of TCP/IP stack. It consumes much less resource and provides high routing performance by providing routing support using Openflow rules only.

[Eric/James] Need numbers to justify the adventage once we have our POC implemented.

[Eric/James] is looking at how dragonflow use openflow. Next step is to submodule ovs repro and try to build an app similar to ovs-ofctl, unless we want to integrate ovs-ofctl into aca in one shot.


== Workflow on Router programming

[Eric] What are the steps to program our router/DVR, FIP manager, SNAT manager


== Virtual Router Implementation

A virtual router will be implemented as part of ACA. When Alcor Controller sends down the goal state messages to compute hosts, ACA will create the in memory virtual router object and also the (virtual) gateway ports connected to it. ACA will then configure routing rules in the virtual router object. 

To implementation in virtual router as DVR in compute node, it needs to have:

. virtual router object
. virtual gateway interface(s) connected
. host virtual router MAC that's unique in the region

TO support L3 routing, ACA will program two sets of rules: the essential set, and the on-demand set.

=== Openflow Essential Rules

The Openflow essential rules are programmed as soon as virtual router information is pushed down to ACA regardless of traffic. We need them to support:

. Intra-subnet traffic (ports in the same subnet that doesn't need routing), send using NORMAL path
. Traffic destinated to one the virtual router port, first packet send to ACA to program the openflow rule
. ARP and ICMP responder so that controller doesn't need to handle it

=== Openflow On-Demand Rules

For inter-subnet L3 traffic between VMs, the first packet will be sent to controller since the on-demand openflow rules has not been programmed yet. This model is used based on the assumption that most VMs don't talk to each other in the cloud environment. Since we don't want to flood our openflow rule table with ton of entries with large scale setup. We have this on-demand model to program the needed rule when needed.

With the first packet sends to CONTROLLER, ACA is acting as the openflow controller and look up its router objects. ACA will find the matching router and then program the corresonding openflow rules on the local machine. Once the openflow rules have been programmed, ACA will simply send the first packet back to OVS to route using the on-demand openflow rule just programmed. 

In order to keep the set of openflow rules lean and small as we scale. The on-demand rule will have an idle timeout of 60s. That means all the ongoing traffic will keep the rule alive, but if there is no traffic hitting the on-demand rule for 60s. The particular on-demand will be removed and any new traffic will hit the essential first packet rule agent to perform the on-demand rule programming. The idle timeout of 60s is the default and can be configured in ACA.

=== ACA Virtual Router Packet Flow

[Eric]  prototype rules

Table Triage: (openflow table 0)

If nothing matches, send to Table Packet Classifier

Table Packet Classifier: (openflow table 50)

. if ARP, send to Table ARP Responder
. if ICMP, send to Table ICMP Responder
. [Eric/James] need special handling for broadcast/multicast? or simply send to Normal?
. else send to Table Forwarding

Table ARP Responder: (openflow table 51)

. if local VLAN and ARP target IP matches an openflow rule, send ARP response
. else send to Normal path

Table ICMP Responder: (openflow table 52)

. if local VLAN and ICMP target matches an openflow rule, send ICMP response
. else send ICMP error

[Eric/James] should we send the ICMP to controller?

Table Forwarding: (openflow table 55)

. (on demand rule) if inter-subnet communication matches an openflow rule, perform L3 forwarding, programmed in last 60s
. (L3 essential rule) if segment ID and destination L3 subnet matches an openflow rule, send to ACA
. (L2 essential rule) if local vlan and local subnet matches an openflow rule, send to Normal path
. else send to Table Public, this is traffic to external

=== Gateway port

In order for two virtual subnets/networks to communicate with each other, both subnets needs to have a gateway port connects to a router instance, similar to how physical network works. 

For a regular port used by VM/Container, the linux network device and OVS port is created by Nova agent on the compute node. For gateway port, ACA will create a virtual gateway port inside its virtual router implementation.


== Example Configuration?

Picture of how our controller/network/compute node looks like.

image::example-configuration.jpg[]


== E2E Packet flows without DVR (neutron namespace/iptable way)

=== Case 1: East-west for instances on different compute hosts on different networks

Project network 1

* Network: 192.168.1.0/24
* Gateway: 192.168.1.1 with MAC address MAC_G1

Project network 2

* Network: 192.168.2.0/24
* Gateway: 192.168.2.1 with MAC address MAC_G2

Compute node 1

* Instance 1: 192.168.1.11 using project network 1

Compute node 2

* Instance 2: 192.168.2.11 using project network 2

In Compute Node 1

. Instance 1 sends a packet to instance 2
. Instance 1 tap interface forwards packet to br-int. The packet contains destination mac MAC_G1 because the destination resides on another network
. br-int adds VLAN tag for project network 1
. For VxLAN/GRE project networks, br-int forwards the packet to br-tun
. br-tun wraps the packet in VxLAN or GRE tunnel and adds a tag (VNI) to identify project network 1
. br-tun forwards the packet to network node via the tunnel interface

In Network Node

. For VxLAN and GRE project networks, tunnel interface forward the packet to br-tun
. br-tun unwraps the packet and adds VLAN tag for project network 1
. br-tun forwards the packet to br-int
. br-int removes the VLAN tag and forwards the packet to qr-1 on qrouter namespace, since qr-1 contains the project network 1 gateway IP 192.168.1.1 with MAC_G1
. qrouter namespace routes packet to qr-2 which contains project network 2 gateway IP 192.168.2.1 with MAC_G2
. qrouter namespace forwards the packet to br-int
. br-int adds the VLAN tag for project network 2
. For VxLAN/GRE project networks, br-int forwards the packet to br-tun
. br-tun wraps the packet in VxLAN or GRE tunnel and adds a tag (VNI) to identify project network 1
. br-tun forwards the packet to compute node 2 via the tunnel interface

In Compute Node 2

. For VxLAN and GRE project networks, tunnel interface forward the packet to br-tun
. br-tun unwraps the packet and adds VLAN tag for project network 2
. br-tun forwards the packet to br-int
. br-int forwards the packet to tap inetrface on instance 2

=== Case 2: North-south for instances with a fixed IP address

External network

* Network: 10.213.0.0/24
* IP allocation 10.213.0.101 to 10.213.0.200
* Network router interface 10.213.0.101

Project network 1

* Network: 192.168.1.0/24
* Gateway: 192.168.1.1 with MAC address MAC_G1

Compute node 1

* Instance 1: 192.168.1.11 using project network 1

In Compute Node 1

. Instance 1 sends a packet to an external host
. Instance 1 tap interface forwards packet to br-int. The packet contains destination mac MAC_G1 because the destination resides on another network
. br-int adds VLAN tag for project network 1
. For VxLAN/GRE project networks, br-int forwards the packet to br-tun
. br-tun wraps the packet in VxLAN or GRE tunnel and adds a tag (VNI) to identify project network 1
. br-tun forwards the packet to network node via the tunnel interface

In Network Node

. For VxLAN and GRE project networks, tunnel interface forward the packet to br-tun
. br-tun unwraps the packet and adds VLAN tag for project network 1
. br-tun forwards the packet to br-int
. br-int removes VLAN tag and forwards the packet to qr-1 on qrouter namespace, since qr-1 contains the project network 1 gateway IP 192.168.1.1 with MAC_G1
. iptable service perform SNAT on the packet using qg interface as the source IP, qg contains external network router interface 10.213.0.101, and send it to the gateway IP on the provider network
. qrouter namespace forwards the packet to br-int via qg
. br-int adds VLAN tag and forwards the packet to br-ex
. br-ex swaps internal VLAN tag to actual VLAN tag, and forwards the packet to external network via the external interface

Note: Return traffic follows similar steps in reverse.

=== Case 3: North-south for instances with a floating IP address

External network

* Network: 10.213.0.0/24
* IP allocation 10.213.0.101 to 10.213.0.200
* Network router interface 10.213.0.101

[Eric] do we need to make 10.213.0.101 as a gateway interface so that external traffic can be routed to it?

Project network 1

* Network: 192.168.1.0/24
* Gateway: 192.168.1.1 with MAC address MAC_G1

Compute node 1

* Instance 1: 192.168.1.11 using project network 1, and floating IP 10.213.0.102

In Network Node

. an external host sends a packet to instance 1 using its floating IP 10.213.0.102
. external interface forwards the packet to br-ex
. br-ex swaps actual VLAN tag with internal VLAN tag, and forwards the packet to br-int
. br-int forwards the packet to qg in qrouter namespace, since qg contains instance 1 floating IP 10.213.0.102
. iptable service perform DNAT on the packet with instance 1 fixed IP 192.168.1.11  
. qrouter namespace forwards the packet to br-int via qr-1 since it contains the project network 1 gateway IP 192.168.1.1 with MAC_G1
. br-int adds the VLAN tag for project network 1
. For VxLAN/GRE project networks, br-int forwards the packet to br-tun
. br-tun wraps the packet in VxLAN or GRE tunnel and adds a tag (VNI) to identify project network 1
. br-tun forwards the packet to compute node 1 via the tunnel interface

In Compute Node 1

. For VxLAN and GRE project networks, tunnel interface forward the packet to br-tun
. br-tun unwraps the packet and adds VLAN tag for project network 1
. br-tun forwards the packet to br-int
. br-int forwards the packet to tap interface on instance 1

Note: Return traffic follows similar steps in reverse, but the network node performs SNAT on traffic passing from instance to external network.


== E2E Packet flow DVR (Alcor openflow way)

[Eric] to provide picture for Case 1

=== Case 1: East-west for instances on different compute hosts on different networks

Project network 1

* Network: 192.168.1.0/24
* Gateway: 192.168.1.1 with MAC address MAC_G1

Project network 2

* Network: 192.168.2.0/24
* Gateway: 192.168.2.1 with MAC address MAC_G2

Compute node 1

* Instance 1: 192.168.1.11 using project network 1

Compute node 2

* Instance 2: 192.168.2.11 using project network 2

prerequistite

. needed DVR instance(s) created in ACA
. DVR gw interface macs programmed as openflow rule to route traffic to ACA
. ACA has route programmed in all DVR instance(s)

In Compute Node 1

. Instance 1 sends a packet to instance 2
. Instance 1 tap interface forwards packet to br-int. The packet contains its gateway destination MAC_G1 because the destination resides on another network
. br-int adds VLAN tag for project network 1
. br-int removes the VLAN tag and forwards the packet to its gateway MAC_G1
. packet sends to ACA based on openflow rule for MAC_G1 and routes to project network 2
. ACA sends the packet to br-int (to be confirmed)
. br-int adds VLAN tag for project network 2
. For VxLAN/GRE project networks, br-int forwards the packet to br-tun
. br-tun wraps the packet in VxLAN or GRE tunnel and adds a tag (VNI) to identify project network 2
. br-tun forwards the packet to compute node 2 via the tunnel interface

In Compute Node 2

. For VxLAN and GRE project networks, tunnel interface forwards the packet to br-tun
. br-tun unwraps the packet and adds VLAN tag for project network 2
. br-tun forwards the packet to br-int
. br-int forwards the packet to tap inetrface on instance 2

Note: Return traffic follows similar steps in reverse except Compute Node 2 will be using its own local DVR to route from project network 2 to project network 1


=== Case 2: North-south for instances with a fixed IP address

External network

* Network: 10.213.0.0/24
* IP allocation 10.213.0.101 to 10.213.0.200

Project network 1

* Network: 192.168.1.0/24
* Gateway: 192.168.1.1 with MAC address MAC_G1
* SNAT interface: 192.168.1.3 SNAT interface on network node, with external IP 10.213.0.102

Compute node 1

* Instance 1: 192.168.1.11 using project network 1

In Compute Node 1

. Instance 1 sends a packet to an external host
. Instance 1 tap interface forwards packet to br-int. The packet contains destination mac MAC_G1 because the destination resides on another network
. br-int adds VLAN tag for project network 1
. br-int removes the VLAN tag and forwards the packet to its gateway mac MAC_G1 in DVR namespace
. DVR routes the packet to the ip of SNAT namespace in the network node
. For VxLAN/GRE project networks, br-int forwards the packet to br-tun
. br-tun wraps the packet in VxLAN or GRE tunnel and adds a tag (VNI) to identify project network 1
. br-tun forwards the packet to network node via the tunnel interface

In Network Node

. For VxLAN and GRE project networks, tunnel interface forward the packet to br-tun
. br-tun unwraps the packet and adds VLAN tag for project network 1
. br-tun forwards the packet to br-int
. br-int removes VLAN tag and forwards the packet to SNAT namespace
. iptable service perform SNAT on the packet using its interface as the source IP
. SNAT namespace routes the packets to provider networks default gw, and forwards the packet to br-int
. br-int adds VLAN tag and forwards the packet to br-ex
. br-ex swaps internal VLAN tag to actual VLAN tag, and forwards the packet to external network via the external interface

[Liguang/Eric/James] to discuss a new design without using network node, use a shared external IP, and change to openflow way.

Note: Return traffic follows similar steps in reverse

=== Case 3: North-south for instances with a floating IP address

External network

* Network: 10.213.0.0/24
* IP allocation 10.213.0.101 to 10.213.0.200
* Network router interface 10.213.0.101

[Eric] how can external traffic can be routed to 10.213.0.101 even for floating IP 10.213.0.102?
[answer] FIP namespace does proxy arp to response to any arp request for any floating IP addresses including 10.213.0.102.

Project network 1

* Network: 192.168.1.0/24
* Gateway: 192.168.1.1 with MAC address MAC_G1

Compute node 1

* Instance 1: 192.168.1.11 using project network 1, and floating IP 10.213.0.102

In Compute Node 1

. an external host sends a packet to instance 1 using its floating IP 10.213.0.102
. external interface forwards the packet to br-ex
. br-ex swaps actual VLAN tag with internal VLAN tag, and forwards the packet to br-int
. br-int removes VLAN tag and forwards the packet FIP namespace, this interface responds to any ARP requests for the instance floating IPv4 address
. FIP namespace routes the packet to DVR namespace to qg in qrouter namespace, since qg contains instance 1 floating IP 10.213.0.102
. iptable service perform DNAT on the packet from instance 1 floating IP 192.168.1.11 to fixed IP 192.168.1.11, using the destination mac of instance 1
. qrouter namespace forwards the packet to br-int via qr-1 since it contains the project network 1 gateway IP 192.168.1.1 with MAC_G1
. br-int adds the VLAN tag for project network 1, and forwards the packet to tap interface on instance 1

[Liguang/Eric/James] to discuss this design, and change to openflow way.

Note: Return traffic follows similar steps in reverse, but the network node performs SNAT on traffic passing from instance to external network. Below is the complete flow:

In Compute Node 1

. Instance 1 sends a packet to an external host
. Instance 1 tap interface forwards packet to br-int. The packet contains destination mac MAC_G1 because the destination resides on another network
. br-int adds VLAN tag for project network 1, and sends to packet to DVR namespace which contains mac MAC_G1
. iptable service perform SNAT on the packet from instance 1 fixed IP 192.168.1.11 to 10.213.0.102
. DVR namespace routes the packet to FIP namespace via a direct veth pair
. FIP namespace routes the packet to 10.213.0.1, which is the default gw for provider network, sends it to br-int
. br-int switches the traffic to br-ex
. br-ex internal VLAN tag is stripped and replaced with actual VLAN tag used in provider network


== L2 Population Optimization

It is an optional feature to prevent the flooding of ARP packet in the datacenter. Since Neutron server is aware of all virtual mac and virtual IP mappings, we can use that information to prepopluation forwarding entries on all tunnel bridges.

[Eric/James] How does the openflow rule look like? Currently table 20 in openstack.

For scenario with two local ports on the same compute host, connected to the same br-int. The current thinking is to follow the same approach. That's to let the arp packet flood to br-tun, using the prepopulated br-tun's arp responder openflow rule. 

[Eric/James] needs to confirm this and check neutron


== Comparison with Neutron router/DVR

How can we do better than the current Openstack setup (DVR)?


== Pending Items

. what happen if host crashed, do we just leverage the ovsdb stored data? Or we ask the Alcor controller for the whole set of configuration upon restart?

. how does Neutron router manages connection flows with HA router?


[bibliography]
== References

- [[[neutron-flows,1]]] https://docs.openstack.org/neutron/train/admin/deploy-ovs-selfservice.html
- [[[neutron-flows-old,2]]] https://docs.openstack.org/liberty/networking-guide/scenario-classic-ovs.html
- [[[neutron-dvr,3]]] https://docs.openstack.org/neutron/train/admin/deploy-ovs-ha-dvr.html