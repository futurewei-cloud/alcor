= Message Queue Subsystem Design - Under Development
Xuwei Yang, Liguang Xie <lxie@futurewei.com>
v0.1, 2021-01-24
:toc: right
:imagesdir: ../../images

NOTE: This document is under development

== Overview

Alcor provides messaging services for controller and agent communication. Messaging services consists of two channels. GRPC and pulsar MQ.
//[.lead]

//Choosing the right data store system is always the key of developing any data-intensive application including Alcor control plane.
//The choice is not that obvious though.
//There are so many database and cache systems on the market with various characteristics as they are designed to
//meet different requirements of different applications.
//
//In this design spec, we go through our system requirements including scalability, availability,
//durability, and performance.
//Secondly, we review existing distributed database and cache solutions,
//discuss their data model, license, and community support, and summarize the pros and cons of each solution.
//We then zoom in on selective databases and compare their features, characteristics and applicable applications.
//Based on the above information, we match our system requirements with the available solutions, and propose architectural design.

[#system-requirements]
== System Requirements
1. DPM can split the network configuration into multiple goal states. DPM can appropriately choose MQ or GRPC channel to send goal state to the corresponding ACA.
2. When ACA configures goal state successfully, it returns success reply. When DPM receives replies of all goal states for a network configuration, it returns successful configuration to the caller
3. Messaging services can support 1000,000 data compute nodes.
4. Working together with other services including Node, Port, VPC, Data Plane managers.
5. Fast, reliable and scalable

== Design
=== Architecture
image::MessageArchitecture.png[]

We have two architectural design options.

==== Design Option: MQ (Node Group Topic)
===== Workflow
image::file.png[]
===== Data Schema for Design Option: MQ (Node Group Topic)
image::dataSchema1.png[]

==== Design Option: MQ (VPC Topic) + GRPC
===== Workflow
image::vpcGroup.png[]

===== Workflow UML
image::MQportcreate.png[]

===== MQ topic subscription or unsubscription
image::MQsubscribe.png[600,400]
[#FeatureComp]



===== Channel & MQ topic Selection for each goal state
image::channelType.png[]
[width="100%",cols="<.^,^.<,^.<",options="header"]
|====================
|Category| Topic | Examples
|Unicast goal state| unicast topic | port bind、dhcp create、route info
|Multicast goal state| multicast topic | neighbor table
|Group multicast goal state| unicast topic & multicast topic | security group metadata & rule
|====================

===== Channel Selection Algorithm
image::channelSelectionAlg.png[]
image::algProcess.png[]
image::channelSelection.png[]



===== Data Schema for Design Option: MQ (VPC Topic) + GRPC
image::dataSchema2.png[]

=== Task Allocation
[width="100%",cols="<.^,^.<",options="header"]
|====================
|Name| Task
|Min Chen| Interaction among managers (steps 1-5 in Workflow UML:select MQ Channel & select GRPC channel); Channel selection ALG (step 6 in Workflow UML:select MQ Channel & select GRPC channel)
|Luyao Luo| ACA subscript/unsubscript (steps 8,9 in Workflow UML:select MQ Channel & select GRPC channel)
|Jiawei Liu| subscript/unsubscript API (step 7 in Workflow UML:select MQ Channel & select GRPC channel); Goalstate send (steps 10,11 in Workflow UML:select MQ Channel; step 10 in Workflow UML:select GRPC channel)
|====================



//Note: * means that the feature is available only in the enterprise edition.

//=== Review of Cache Store
//
//[width="100%",options="header"]
//|====================
//|Cache|Type|Pros|Cons|License
//|Option 1: Memcached
//|Cache service
//|
//|
//|
//
//|Option 2: Redis
//| Cache service
//a|
//- Support HA cluster
//- Data persistence
//- Support a variety of data structures ranging from bitmaps, steams, and spatial indexes
//|
//| BSD
//
//|Option 3: LevelDB | In-memory cache | | |
//
//|Option 4: Riak
//| Distributed key-value database
//a|
//- Distributed design
//- Advanced local and multi-cluster replication
//|
//|
//|====================
//
//Note: Cache is optional at this point.
//Our plan is to first conduct a performance analysis for various database storage solutions in terms of throughput, latency and other factors.
//If TPS couldn't satisfy our target performance requirement, we will incorporate cache in our design.
//
//=== Cache Access Pattern
//
//Cache Aside Pattern: For write operation, we could use cache aside pattern which recommends to delete cache entry,
//instead of resetting cache entry.
//
//Pending item:
//
//* Modify database then remove cache entry (to reduce the possibility of read old data immediate after write and legacy cache)
//* Remove cache entry then modify database (ensure atomic operation)


//[#architecture]
//== Architectural Design
//
//Based on <<system-requirements>> and <<FeatureComp>>,
//Apache Ignite provides a very rich feature set that matches most of our system requirements. Specifically, it offers the following features:
//
//* Standalone distributed database and built-in cache services
//* Strong consistency, distributed ACID transactions and SQL queries
//* Data sharding and cross-shard transacation
//* Proven horizontal scalability to meet our throughput and storage requirement
//* Cross-DC and cross-AZ geo replication for AZ-resilient HA
//* In-memory processing capabilities applicable for read heavy workload application while offering low latency for writes
//* Rolling upgrade without downtime
//* Collocated joins and non-collocated joins
//* In-memory indexing
//
//Regarding performance and storage size,
//the benchmark results with Yardstick <<ignite_benchmark>> shows that Ignite could reach up to 1/3 million Ops and less than 1 millisecond latency with four average server machines (2x Xeon E5-2609 v4 1.7GHz, 96 GB RAM).
//The catch is that the benchmark is conducted by only one client node with 128 client threads, which does not consider network round trip time in the scenarios where 2-phase commit is applied.
//
//The comparison results with Cassandra <<ignite_cassandra>> used a more distributed benchmark YCSB with three server nodes (same server configuration as used in Yardstick).
//In a 256 client threads setup, Ignite could reach up to 300K READ Ops and 150K READ+UPDATE Ops.
//
//In short, Ignite fits into read-intensive and mixed workloads.
//With data shading support, the throughput and latency data is expected to meet our system requirements.
//Its maximum reliable dataset size could reach up to hundreds of TBs, which provides sufficient margin to support fast-growing pace of public cloud.
//
//TIP: To get more details about how to scale Ignite cluster to meet the storage requirements,
//refer to <<capacity>>.

//We have two architectural design options.
//
//[#MQ-only-option]
//=== Design Option: Message Queue Only
//
//MQ only
//
//[#MQ-NFS-option]
//=== Design Option: Message Queue & NFS
//
//MQ + NFS
//
//[#MQ-self-learning]
//=== Design Option: Selective Messaging & Host Self Learning
//
//Selective messaging through MQ + host self learning

[bibliography]
== References
- [[[ignite_home,1]]] Apache Ignite: https://ignite.apache.org/
- [[[pulsar_home,2]]] Apache Pulsar: http://pulsar.apache.org/
