= Data Store and Cache Subsystem Design
Liguang Xie <lxie@futurewei.com>
v0.1, 2019-10-27
:toc: right
:imagesdir: ../../images

== Overview

//[.lead]

Choosing the right data store system is always the key of developing any data-intensive application including Alcor control plane.
The choice is not that obvious though.
There are so many database and cache systems on the market with various characteristics as they are designed to
meet different requirements of different applications.

In this design spec, we go through our system requirements including scalability, availability,
durability, and performance.
Secondly, we review existing distributed database and cache solutions,
discuss their data model, license, and community support, and summarize the pros and cons of each solution.
We then zoom in on selective databases and compare their features, characteristics and applicable applications.
Based on the above information, we match our system requirements with the available solutions, and propose architectural design.

[#system-requirements]
== System Requirements

=== Must-have features
* Standalone without dependency on other data-center services
* Horizontal scaling along with region expansion
* Strong consistency and ACID transaction
* High availability & Fault tolerance with graceful disaster handling
* Highly performant for read heavy workload while maintaining low latency for writes
* Support rolling upgrade without downtime

=== Nice-to-have features
* Built-in cache support
* Scale to 100s nodes in one _database_ cluster in proven production environment
* Cross-shard transaction (one applicable scenario is VPC peering when two VPCs could land on different data shards).
* Cross-DC and cross-AZ geo replication
* Support ANSI-99 SQL
* Support collocated joins and non-collocated joins
* Support primary/secondary index
* No need for warm-up time after cluster node restart
* Event watches and leases

=== Throughput and Data Storage Requirements
NOTE: The requirements given below are for VPC controller services.
Similar analysis could be done for other controllers.

* Traffic pattern
** The control plane traffic pattern is expected to be READ intensive with most majority of GURD calls are READ.

* Throughput target for data storage subsystem
** The target is 1 million read/write operations per second.
** Estimated size of a middle-sized region is 500,000 node with 100 ports per node with consideration of multi-port container and VMs.
Thus, the estimated total number of ports is ~50 millions.
** The estimated number of port-related resources (e.g. security groups, ACL, routes, DNS records and subnets) is of the same magnitude.
Doubling the port number would make the total number of network resources at *O*(100 millions).
** The estimated rate of system concurrency is 1% including query, insert and update,
which results in the estimated throughput of 1 million read/write operations per second.

* Storage size
** The storage size of network resource depends on multiple factors including the size of VPC and subnets,
and customer-specified fields (e.g. security rules and routes).
The size varies significantly ranging from a few _KBs_ to hundreds of _KBs_.
** On average, we look into the estimated size of *O*(10 _KB_) which could be subject to adjustment.
** Given the estimated total number of resources at *O*(100 millions), we look into a system storage size between 1 _TB_ and a few _TBs_.

== Review of Distributed Databases

Based on the expected scale and throughput, we select a few popular distributed databases from different categories including distributed SQL,
key-value store, no-SQL columnar database, document store, and graph store.
We review the data model, license, community support and discuss their pros and cons.

If you are already familiar with existing solutions on the market, please skip <<ReviewDatabase>> and refer to

- <<FeatureComp>> for more deep dive of selective distributed databases and comparison of their features, characteristics and applicable applications;
- <<#architecture>> for our system design proposal.

[#ReviewDatabase]
=== High-Level Overview of Popular Databases
[width="100%",cols="1,1,3,1,1,1", options="header"]
|====================
|Name|DB model and type|Pros|Cons| Adoption and Community Support | License

|Option 1: Apache Ignite <<ignite_home>>
a|
- Multi-model database supporting both key-value and SQL for modeling and accessing data
- Developed by _Java_
a|
- Strongly consistent distributed database
- Support distributed ACID transactions, SQL queries, on-disk persistence.
- Provide strong processing APIs for computing on distributed data
- Cross DC and cross geo-region support
a|
- Supported programming languages are limited - _JAVA_, _C++_ and _C#_
a|
- Top 5 Apache project by commits
- Top 3 most active Apache mailing lists
| Apache 2.0

|Option 2: ETCD <<etcd>>
a|
- No-SQL KV store
- Developed by _Go_
a|
- Strongly consistent KV store (via Raft protocol)
- Support watch of keys or directories for changes
- Cross-platform support, small binaries
a|
- Unable to scale horizontally due to lack of data sharding
- Limited data store up to a few _GB_ <<etcd_data_model>>
a|
- Great community support backed by CNCF.
- Native storage system for Kubernetes
| Apache 2.0

|Option 3: Apache Cassandra <<cassandra>>
a|
- No-SQL columnar database developed by _JAVA_
- Eventual/ tuneable consistency level for Read/Write
- Consistent hashing for mapping keys to servers/nodes
a|
- Fast write performance
- Distributed and decentralized design (Gossip peer-to-peer protocol for distributed node management)
- Cross DC and cross geo-region support
- Large-scale deployment up to over 75,000 nodes
- Flexible scheme with CQL query support
a|
- Doesn't support ACID transaction (only AID at row/partition level)
a|
- Apache open source project originally sprung out of Facebook.
- Contributors include Apple, Linkedin, Twitter.
- Apple had the biggest Cassandra instance with 75,000+ nodes and stored more than 10 petabytes of data <<cassandra_data>>
| Apache 2.0

|Option 4: ScyllaDB <<scylla>>
a|
- Cassandra-compatible wide columnar store
- Rewrite Cassandra in _C++_
- Claimed to be the fastest NoSQL database with 99% tail latency less than 1 _msec_
a|
- Highly-performant (efficiently utilizes full resources of a node and network; millions of IOPS per node)
- Highly-available (peer-to-peer, no single-point-of-failure, active-active)
- Share many features of Cassandra like horizontal scaling, tunable consistency model and built-in geo replication
a|
- Relatively low adoption rate
a|
- Open source project adopted by Comcast, Grab, Yahoo! Japan etc.
- Not donated to any open source foundation
a|
- Open source is based on Apache GPL v3.0
- Scylla Enterprise is subscription-based
- Scylla Cloud is a managed DBaaS with various pricing models including annual, monthly and hourly

|Option 5: Apache Hbase <<hbase>>
a|
- No-SQL columnar database
- Developed by _Java_
a|
- Provides Google's Bigtable-like capabilities on top of Apache Hadoop
- Offer strong consistency
- Support structured storage for large amounts of data (on top of HDFS)
a|
- Centralized master-based architecture could cause single point of failure <<hbase_cassandra>>
- Lack of query language like Cassandra
a|
- Open source project adopted by Netflix, Flipkart, Facebook etc.
- Backup by Cloudera
| Apache 2.0

|Option 6: MangoDB <<mongodb>>
a|
- No-SQL document store developed in _C++_
- Use _JSON_ alike documents to store data
a|
- Schema-free design provides flexibility and agility on various data type
//- Fields can vary from document to document and data structure can be changed over time
- multi-document ACID Transactions with snapshot isolation
- Built in high availability, horizontal scaling, and geo distribution
a|
- MapReduce implementations remain a slow process <<mangodb_compare>>
- MongoDB suffers from memory hog issues as the databases start scaling
a|
- Great community support.
- Most widely used document-oriented database (by Google, Facebook, eBay, SAP etc.)
a|
- Community edition is under Server Side Public License (SSPL) v1 after Oct. 16, 2018, otherwise Apache GPL
- Enterprise edition is supported by MongoDB, Inc.

//|Option 7: Apache CouchDB
//a|
//- No-SQL document store
//- Store data as JSON documents and uses JavaScript as query language
//| | | |

|Option 7: Neo4j <<neo4j>>
a|
- No-SQL graph database developed in _Java_
- Data stored in documents with a focus on relationship between individual documents
a|
- ACID-compliant DBMS
- Most popular graph-oriented database as of this writing.
a|
- Unsupported data sharding
| Adopted by Ebay, Walmart, NASA etc.
a|
- Community edition is under GPL v3 license.
- Enterprise edition is supported by Neo4j, Inc.
|====================

[#FeatureComp]
=== Feature Comparision among Selective Databases

[width="100%",cols="<.^,^.<,^.<,^.<,^.<",options="header"]
|====================
|Name| Apache Ignite | ETCD | Apache Cassandra | ScyllaDB

|Applicable application
| Read-intensive or mixed application <<ignite_cassandra>>
| Application requires infrequent data update (e.g. metadata) and reliable watch queries <<etcd_data_model>>
| Write-intensive application <<ignite_cassandra>>
| Application requires ultra-low latency and extreme throughput

|Distributed design
|Yes
|Yes
|Decentralized and master-less
|Decentralized and master-less

|Data sharding
|Yes (via distributed hashing table)
|No (data sharding unsupported)
|Yes
|Yes

|Strong consistency
|Yes
|Yes (consensus achieved through raft protocol)
|No. Eventual/tuneable consistency
|No. Eventual consistency and tuneable per query

|ACID transaction
|Yes (distributed transaction via improved 2-phase commit)
|Yes (single shard ACID)
|No. Light-weighted transaction (LWT)
|No (with a roadmap ofsupporting CQL Light-weight transactions (LWT) in 3.x.)

|Cross-shard transaction
|Yes (with the support of transaction coordinator)
|No
|No
|No

|Concurrency modes
|Pessimistic & optimistic
|Caller responsible for acquiring explicit lock (via ETCD v3 service Lock)
|Hybrid of optimistic and pessimistic (switched to pessimistic in high contention on a single partition)
|

|Isolation levels
|Read Committed & Repeatable Read & Serializable
|Serializable isolation by MVCC
|Serializable (LWT on a per-row basis through Paxos)
|Serializable isolation

|Multiversion Concurrency Control
|Yes (Snapshot isolation is in Beta at v2.7,
only support pessimistic concurrency and Repeatable Read isolation)
|Yes (A multiversion persistent & immutable kv store with past versions of key-value pair preserved and watchable)
|No (with last-writer-wins semantics)
|Yes

|Data persistence
|Support WAL and check pointing
|Data stored in a persistent b+ tree
|SSTable (ordered and immutable)
|SSTable (ordered and immutable)

|In-memory cache capabilities
|Yes (data and indexes stored in managed off-heap regions in RAM and outside of Java heap)
|No
|No (data stored in a memory buffer before flushing out to disk)
|Yes (In-memory tables to reduce read latency for mostly read workload)

|ANSI-99 SQL
|Yes (via ODBC/JDBC APIs to Ignite, including both DDL and DML)
|No
|No but support SQL-like DML and DDL statements (CQL)
|No but support SQL-like DML and DDL statements (CQL)

|Collocated joins
|Yes
|No
|No
|No

|Non-collocated Joins
|Yes
|No
|No
|No

|Geo replication
|Yes * (active-passive and active-active bi-directional replication)
|No
|Yes
|Yes

|Secondary index
|Yes
|Yes (store a secondary index on memory and use btree to match the key to its physical data location)
|Yes (does not use one single type of index clustered on the Primary Key)
|Yes (maintains an index table for the secondary index keys)

|Foreign keys | No | No | No | No

|Event watches/leases/elections
|No (cache interceptors and events?)
|Yes (built-in support)
|No
|No

//|Synchronous replication model
//|
//|Single leader
//|Use Zookeeper for leader election
//|

//|Semi-synchronous
//|
//|Single leader
//|
//|

|Replication logs and mechanism
|Write-ahead log
|Appends a gRPC request to a write-ahead log
|Write-ahead log(?)
|Write-ahead log(?)

|Rolling upgrade
|Enterprise edition support rolling upgrade for minor and maintenance versions of the same major series*
|Yes
|Yes
|Yes

|Maximum reliable database size
|Hundreds of _TBs_
|Several _GBs_ <<etcd_data_model>>
|Apple had the biggest Cassandra instance with 75,000+ nodes and stored more than 10 _PBs_ of data <<cassandra_data>>
|It is reporeted that one Scylla customer runs a 1 _PB_ cluster with 30 nodes <<scylla_data>>

|====================

Note: * means that the feature is available only in the enterprise edition.

//=== Review of Cache Store
//
//[width="100%",options="header"]
//|====================
//|Cache|Type|Pros|Cons|License
//|Option 1: Memcached
//|Cache service
//|
//|
//|
//
//|Option 2: Redis
//| Cache service
//a|
//- Support HA cluster
//- Data persistence
//- Support a variety of data structures ranging from bitmaps, steams, and spatial indexes
//|
//| BSD
//
//|Option 3: LevelDB | In-memory cache | | |
//
//|Option 4: Riak
//| Distributed key-value database
//a|
//- Distributed design
//- Advanced local and multi-cluster replication
//|
//|
//|====================
//
//Note: Cache is optional at this point.
//Our plan is to first conduct a performance analysis for various database storage solutions in terms of throughput, latency and other factors.
//If TPS couldn't satisfy our target performance requirement, we will incorporate cache in our design.
//
//=== Cache Access Pattern
//
//Cache Aside Pattern: For write operation, we could use cache aside pattern which recommends to delete cache entry,
//instead of resetting cache entry.
//
//Pending item:
//
//* Modify database then remove cache entry (to reduce the possibility of read old data immediate after write and legacy cache)
//* Remove cache entry then modify database (ensure atomic operation)


[#architecture]
== Architectural Design

Based on <<system-requirements>> and <<FeatureComp>>, Apache Ignite provides a very rich feature set that matches most of our system requirements. Specifically, it offers the following features:

* Standalone distributed database and built-in cache services
* Strong consistency, distributed ACID transactions and SQL queries
* Data sharding and cross-shard transacation
* Proven horizontal scalability to meet our throughput and storage requirement
* Cross-DC and cross-AZ geo replication for AZ-resilient HA
* In-memory processing capabilities applicable for read heavy workload application while offering low latency for writes
* Rolling upgrade without downtime
* Collocated joins and non-collocated joins
* In-memory indexing

Regarding performance and storage size,
the benchmark results with Yardstick <<ignite_benchmark>> shows that Ignite could reach up to 1/3 million Ops and less than 1 millisecond latency with four average server machines (2x Xeon E5-2609 v4 1.7GHz, 96 GB RAM).
The catch is that the benchmark is conducted by only one client node with 128 client threads, which does not consider network round trip time in the scenarios where 2-phase commit is applied.

The comparison results with Cassandra <<ignite_cassandra>> used a more distributed benchmark YCSB with three server nodes (same server configuration as used in Yardstick).
In a 256 client threads setup, Ignite could reach up to 300K READ Ops and 150K READ+UPDATE Ops.

In short, Ignite fits into read-intensive and mixed workloads.
With data shading support, the throughput and latency data is expected to meet our system requirements.
Its maximum reliable dataset size could reach up to hundreds of TBs, which provides sufficient margin to support fast-growing pace of public cloud.

TIP: To get more details about how to scale Ignite cluster to meet the storage requirements,
refer to <<capacity>>.

We have two architectural design options.

[#cross-az-design]
=== Design Option: Cross-AZ Architecture
Option 1 is cross-AZ design, as illustrated in the following diagram.
Controller services and database/cache clusters are deployed across Availability zones.
Each availability zone has multiple pods/replicas of each service,
and multiple Ignite nodes for high availability and high resilience.
We leverage the geo-replication features provided by Ignite,
and enable data center replication feature (available in Enterprise/Ultimate Edition up to 31 data centers <<crossaz>>, and turned off by default)
which supports both active-active and active-passive modes for replication.

The advantage of option 1 is AZ resilience and fast read operations.
If any AZ or any data center goes down, control plane service remains up and fully capable of assuming its load and data.
Secondly, read operation is expected to have ultra-low latency due to data locality
-- service could always reach up-to-date data
from Ignite nodes deployed at the same AZ as service node --
and in-memory cache capability.

The downside is that data replication takes more time when data update is across the AZ boundary
(in a matter of a few _ms_ by common industrial practise) and lower write throughput.
This could be countered by Ignite's in-memory capabilities and persistence model that a write operation
could be acknowledged right after the data reaches RAM and operation is recorded in write-ahead log.

image::database.JPG["Database and Cache Architecture", width=1024, link="database.JPG"]

[#single-az-design]
=== Design Option: Single-AZ Architecture

NOTE: The discussion targets a multi-AZ setup but this architecture is also applicable to a region with single AZ.

Option 2 is a simplified version of Option 1, and mostly applies when the open source version of Ignite is used.
Database/cache cluster is deployed within one data center while controller continues to be distributed across DC/AZ boundary.

The pros/cons of option 2 is quite as opposite of those of option 1.
There is no AZ resilience for database/cache subsystem (thus the entire control plane) although controller services
could continue to benefit from cross-AZ deployments.
The read/write latency varies depending on how far the service node is from the data node.
When services and data happen to be in the same AZ, we expect to have low latency for both read and write.
Otherwise, latency increases as every call needs to go across AZ boundary.

//
//=== Design Principles
//
//* High availability
//* High read/write throughput (measured in RPS)
//** Add index in read database
//** Use redundant database (for read or write, RW split, or shadow master) to improve HA and increase throughput
//** Add cache
//
//* Consistency
//** Use middleware to read from master in the inconsistent window
//** Read/Write from the same master, and add a shadow master
//
//* Extensibility

=== Service-Aware Data Sharding

As a result of the estimated throughput and storage size,
a single machine (or even a partition with multiple replicas) is
certainly unable to scale to the required high load.
To support horizontal scaling, the service-aware data sharding proposal here leverages a few useful Ignite features,
such as customized affinity function and data collocation.

==== Per-Micro-Service Data Caching and Sharding

Every micro-service maintains its own cache configuration including customized
affinity function (managing the mapping from cache key to partition),
crash-safe affinity (ensuring primary and backup copies located at distinct physical machines),
and partition numbers.
As an example, the VPC cache configuration is listed as follows:

[source,xml]
----
<bean class="org.apache.ignite.configuration.IgniteConfiguration">
    <property name="cacheConfiguration">
        <list>
            <!-- Creating a cache configuration. -->
            <bean class="org.apache.ignite.configuration.CacheConfiguration">
                <property name="name" value="myCache"/>

                <!-- Creating the affinity function with custom setting. -->
                <property name="affinity">
                    <bean class="org.apache.ignite.cache.affinity.rendezvous.RendezvousAffinityFunction">
                        <property name="excludeNeighbors" value="true"/>
                        <property name="partitions" value="2048"/>
                    </bean>
                </property>
            </bean>
        </list>
    </property>
</bean>
----

Below is the proposed cache key to be used in various micro-services in our VPC controller:

[width="100%",options="header"]
|====================
|Micro-Service|Cache/Partition Key|Requirements
|Private IP Allocator|Subnet Id| Subnet-level uniqueness
|Virtual Mac Allocator|MAC address prefix| Regional uniqueness
|Virtual IP Allocator|IP address prefix (Ipv6 and Ipv4)| Global uniqueness
|VPC Manager|VPC Id
a|
- Global uniqueness.
- Manage VNI/Route/ACL/Security Group
|DNS Manager|DNS record id|Regional uniqueness
|Node Manager|Node Id|Regional uniqueness
|====================

==== Service-Managed Data Collocation

In many cases when multiple cache keys are accessed together, control plane can have performance gain for both read and write
if different cache keys are collocated on the same processing node.
By doing so, we avoid costly network trips to fetch data from remote nodes.
For example, we prefer to store a VPC and all of its subnets on the same nodes as they will be accessed pretty frequently.
We can bundle them by giving them the same *affinityKey*.
The example codes are given as follows:

[source,java]
----
public class SubnetKey {
    // Subnet ID used to identify a subnet.
    private String subnetId;

    // VPC ID which will be used for affinity.
    @AffinityKeyMapped
    private String vpcId;
    ...
}
----

To check Affinity key mapping, we could use Ignite's *AffinityFunction* pluggable APIs <<ignite_affinity_apis>>:

- partitions() - Gets the total number of partitions for a cache.
- partition(...) - Given a key, this method determines which partition a key belongs to.
The mapping must not change over time.
- assignPartitions(...) - This method is called every time a cluster topology changes.
This method returns a partition-to-node mapping for the given cluster topology.

//==== Data Routing Algorithm
//[width="100%",options="header"]
//|====================
//|Data Routing Option|Pros|Cons
//|Option 1: Key Range
//|Simple and easy to expand
//|Uneven load distribution
//
//|Option 2: Hash by Key
//|Simple and even load distribution
//|Hard to migrate data during database scale-out
//
//|Option 3: Router-config-server
//|Flexible with decoupling of business logic with routing algorithm
//|Additional query before every database visit
//
//|Option 4: Embed partition information in resource id
//a|
//- Simple and consistent mapping during database scale-out
//- Allow customized mapping from resource id to node
//|
//|====================


=== Native Data Persistence

NOTE: Ignite native persistence is a distributed ACID and SQL-compliant disk store that transparently integrates with Ignite's durable memory.
It is optional, and when turned off,
Ignite becomes a pure in-memory store and could work with many 3rd party databases such as RDBMS, HDFS and NoSQL.
If turned on, Ignite serves as distributed database and cache at the same time.
We opt in the native persistence to take advantage of many benefits brought by native persistence including running SQL queries
on both memory and disk, and quick cluster (re)start without preloading data from the disk into the memory.

With the native persistence enabled, Ignite always stores a superset of data on disk,
and as much as it can in RAM based on the capacity of the latter.
By default, the persistence files are maintained under a shared ${IGNITE_HOME}/work/db directory.
If several cluster nodes are started on a single machine,
every node process will have its persistence files under a uniquely defined subdirectory such as ${IGNITE_HOME}/work/db/node{IDX}-{UUID}.
Both IDX and UUID parameters are calculated by Ignite automatically upon the nodes' startup.

=== Data Replication Mode

Data replication is very useful on availability and performance with the following usage:

- To increase availability and resilience
- To keep data geographically close to the controller services thus reduce latency
- To increase the read throughput

Based on discussion in <<system-requirements>> and <<cross-az-design>>, our system design expects read heavy workload and high availability.
Accordingly, Ignite offers three cache replication models including PARTITIONED, REPLICATED and LOCAL, each with trade-off between availability and performance <<ignite_replication>>.
If the expected availability requirement is AZ resilience, we prefer to use cross-AZ geo replication and REPLICATED cache mode
where all the data is replicated to every node in the cluster.
This cache mode provides the utmost availability of data.
Otherwise, we could use PARTITIONED mode where updates become cheaper compared to REPLICATED mode because only one primary node
(and optionally 1 or more backup nodes) need to be updated for every key.
However, read throughput would drop and reads become more expensive because only certain nodes have the data cached.

As an example, the VPC cache configuration is set as follows:

[source,xml]
----
<bean class="org.apache.ignite.configuration.IgniteConfiguration">
    ...
    <property name="cacheConfiguration">
        <bean class="org.apache.ignite.configuration.CacheConfiguration">
            <!-- Set a cache name. -->
            <property name="name" value="vpcCache"/>
            <!-- Set cache mode. -->
            <property name="cacheMode" value="REPLICATED"/>
            <!-- Other cache configurations. -->
            ...
        </bean>
    </property>
</bean>
----

//Leader-based replication
//
//Popular algorithms for replicating changes between nodes:
//
//- single leader
//- multi leader
//- leaderless
//
//Synchronous vs Asynchronous replication
//
//- configurable option or hardcoded
//- semi-synchronous

//Alcor Replication model
//
//- Each AZ has a primary
//- Semi-synchronous replication within a AZ
//- Asynchronuous replication

//=== High Avaialbility
//As the minimal, the availability requirements
//* Fault tolerance with graceful disaster handling
//** Capable of handling node outages and planned maintenance
//** Zero downtime: keep the system as a whole running despite individual node failure
//
//=== Data Inconsistency Handling
//
//There is synchronization latency between multiple database instances (from leader to follower nodes).
//This could potentially cause inconsistency in the following scenarios:
//
//* Service instance X issues a write/update request to port
//* Service instance Y requests a read/get of the same port, and the request reaches a follower node
//before the synchronization is completed.
//Therefore the data retrieved by instance Y is legacy data.
//* Database synchronization is completed eventually
//
//We consider three options as follows to handle such a scenario:
//[width="100%",options="header"]
//|====================
//|Cache|Pros|Cons
//|Option 1: Ignore differences | Simple working solution for many online services like web searching, message system etc.| Not applicable to scenarios requiring strong consistency
//|Option 2: Read/write goes to a HA master | Common strategy used in microservice design to avoid inconsistency issue| Heavy-loaded master node with limited read throughput. Usually cache is supported to increase the read TPS.
//|Option 3: Selectively reading master in the transition period| A balanced strategy: Prevent inconsistency issue in most cases and avoid overloading master node | Overhead of reading cache before database
//|====================
//
//Details about option 3:
//
//* Write Steps
//
//** Write to the master node
//** Generate a cache key with the following format "db:table:PK" by aggregating db, table name and id
//** Write to a cache and set the entry expiration time as the synchronization latency. e.g. 500 _ms_.
//
//* Read Steps
//** Use the same step to generate the cache key
//** When hitting a cache, read the data from master node
//** Otherwise, read the data from other nodes

[#capacity]
=== Capacity Planning

Based on discussion in <<system-requirements>>, the expected size of dataset is *O(1 TB)*.
In the recommended capacity plan <<ignite_capacity>>, we assume that 30% of index capacity and 33% of data stored in RAM.
We propose to use 2 backup copies in addition to primary one so that a three-AZ region could be covered.
We enable native persistence and assume the SSD over-provisioning rate is 100%.

NOTE: The plan is subject to change when more data points are collected.

The required data capacity in RAM is as follows:
[width="100%",options="header"]
|====================
|Required data capacity|Value (GB)
|Raw data | 1,000
|Primary data capacity | 1,350
|Total data capacity with backups | 4,050
|Total RAM capacity needed | 1,337
|====================

The required data capacity in disk is as follows:
[width="100%",options="header"]
|====================
|Required data capacity|Value (GB)
|Raw data | 4,050
|Total disk capacity needed| 6,885
|====================

The following table lists the number of required servers/machines with a few sample server configurations:
[width="100%",options="header"]
|====================
|Cores| RAM (GB) | Disk(GB) | Number of Required Servers
|16 | 128 | 1,200 (2 x 600 NVMe SSD) | 12
|32 | 256 | 2,400 (4 x 600 NVMe SSD) | 6
|48 | 384 | 3,600 (4 x 900 NVMe SSD) | 4
|====================


[bibliography]
== References

- [[[ignite_home,1]]] Apache Ignite: https://ignite.apache.org/
- [[[etcd,2]]] ETCD: https://etcd.io
- [[[etcd_data_model,3]]] ETCD data model: https://github.com/etcd-io/etcd/blob/master/Documentation/learning/data_model.md
- [[[cassandra,4]]] Apache Cassandra: http://cassandra.apache.org/
- [[[cassandra_data,5]]] Apache Cassandra: Four Interesting Facts https://www.datastax.com/blog/2019/03/apache-cassandratm-four-interesting-facts
- [[[scylla,6]]] Scylla DB: https://www.scylladb.com/
- [[[hbase,7]]] Apache HBase: https://hbase.apache.org/
- [[[hbase_cassandra,8]]] Cassandra vs. HBase: twins or just strangers with similar looks? https://www.scnsoft.com/blog/cassandra-vs-hbase
- [[[mongodb,9]]] MangoDB: https://www.mongodb.com/
- [[[mangodb_compare, 10]]] Cassandra vs. MongoDB vs. Hbase: A Comparison of NoSQL Databases https://logz.io/blog/nosql-database-comparison/
- [[[neo4j,11]]] Neo4j: http://neo4j.com
- [[[ignite_cassandra,12]]] Apache Ignite and Apache Cassandra Benchmarks: The Power of In-Memory Computing https://www.gridgain.com/resources/blog/apacher-ignitetm-and-apacher-cassandratm-benchmarks-power-in-memory-computing
- [[[scylla_data,13]]]Scaling Up versus Scaling Out:
Mythbusting Database Deployment Options for Big Data https://www.scylladb.com/wp-content/uploads/wp-scaling-up-vs-scaling-out.pdf https://www.scylladb.com/wp-content/uploads/wp-scaling-up-vs-scaling-out.pdf
- [[[crossaz,14]]] Gridgain data center replication: https://www.gridgain.com/products/software/enterprise-edition/data-center-replication
- [[[ignite_affinity_apis,15]]] Apache Ignite AffinityFunction: https://ignite.apache.org/releases/latest/javadoc/org/apache/ignite/cache/affinity/AffinityFunction.html
- [[[ignite_replication,16]]] Apache Ignite Partitioning and Replication: https://apacheignite.readme.io/docs/cache-modes
- [[[ignite_capacity,17]]] Apache Ignite Capacity Planning: https://apacheignite.readme.io/docs/capacity-planning
- [[[ignite_benchmark,18]]] GridGain Benchmarks Results: https://www.gridgain.com/resources/benchmarks/gridgain-benchmarks-results
